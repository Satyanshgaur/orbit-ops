{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff940a17",
   "metadata": {},
   "source": [
    "## Preprocessing TEMPO satellite and ground data for model training\n",
    "\n",
    "This is the jupyter notebook guide to merging and preparing data for gradient booster and random forest machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51756577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'humidity' column deleted successfully from weather_data\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Downloads\\phase1_data (3).db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 1. Rename old table and create new one without 'humidity'\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE weather_data_new AS\n",
    "    SELECT id, timestamp, temperature, wind_speed\n",
    "    FROM weather_data\n",
    "\"\"\")\n",
    "\n",
    "# 2. Drop the old table\n",
    "cursor.execute(\"DROP TABLE weather_data\")\n",
    "\n",
    "# 3. Rename new table to original name\n",
    "cursor.execute(\"ALTER TABLE weather_data_new RENAME TO weather_data\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… 'humidity' column deleted successfully from weather_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759ccdc",
   "metadata": {},
   "source": [
    "### Checking null data and preparing for imputation \n",
    "\n",
    "to imputate null data, checking how much of our data is null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Percentage of NULL values in each column:\n",
      "id            0.000000\n",
      "timestamp     0.000000\n",
      "latitude      0.000000\n",
      "longitude     0.000000\n",
      "pm25          2.282863\n",
      "no2          37.720160\n",
      "so2          51.356867\n",
      "o3           67.420062\n",
      "co           51.477814\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Downloads\\phase1_data (3).db\")\n",
    "\n",
    "# Load weather_data into pandas DataFrame\n",
    "df = pd.read_sql(\"SELECT * FROM ground_data\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Calculate percentage of NULL values in each column\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"ðŸ“Š Percentage of NULL values in each column:\")\n",
    "print(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d207f",
   "metadata": {},
   "source": [
    "### Sorting data according to timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id         timestamp  latitude  longitude  pm25   no2  so2     o3     co\n",
      "0    1  2024-08-01 00:00  40.81600  -73.90200   6.9  16.3  NaN  0.028    NaN\n",
      "1   32  2024-08-01 00:00  40.71961  -73.94771  16.1   NaN  NaN    NaN    NaN\n",
      "2  112  2024-08-01 00:00  40.79970  -73.93432  13.5   NaN  NaN    NaN    NaN\n",
      "3  128  2024-08-01 00:00  40.73614  -73.82153  10.5  20.1 -0.4  0.018  0.413\n",
      "4  160  2024-08-01 00:00  40.73000  -73.98400  14.6   NaN  NaN    NaN    NaN\n",
      "âœ… ground_data sorted by timestamp and saved to 'ground_data_sorted.csv'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Downloads\\phase1_data (2).db\")\n",
    "\n",
    "# Read and sort ground_data by timestamp\n",
    "df_ground = pd.read_sql(\"SELECT * FROM ground_data ORDER BY timestamp ASC\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Show first few rows\n",
    "print(df_ground.head())\n",
    "\n",
    "# Save cleaned, sorted data to CSV\n",
    "df_ground.to_csv(\"ground_data_sorted.csv\", index=False)\n",
    "print(\"âœ… ground_data sorted by timestamp and saved to 'ground_data_sorted.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136bc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id         timestamp  latitude  longitude  pm25   no2  so2     o3    co\n",
      "0    1  2023-04-03 00:00    40.816    -73.902   3.5  15.7  0.3  0.034  None\n",
      "1    2  2023-04-06 00:00    40.816    -73.902   6.3  10.4  0.0  0.027  None\n",
      "2    3  2023-04-09 00:00    40.816    -73.902   6.7  28.2  0.4  0.019  None\n",
      "3    4  2023-04-12 00:00    40.816    -73.902   6.9  11.5  0.5  0.046  None\n",
      "4    5  2023-04-15 00:00    40.816    -73.902   9.8  17.3  0.6  0.024  None\n",
      "5    6  2023-04-18 00:00    40.816    -73.902   2.9   4.8  0.1  0.035  None\n",
      "6    7  2023-04-21 00:00    40.816    -73.902   8.5  27.6  0.3  0.022  None\n",
      "7    8  2023-04-24 00:00    40.816    -73.902   3.3   5.4  0.0  0.031  None\n",
      "8    9  2023-04-27 00:00    40.816    -73.902  11.5  19.7  0.1  0.032  None\n",
      "9   10  2023-04-30 00:00    40.816    -73.902   2.9   3.8  0.0  0.041  None\n",
      "10  11  2023-05-03 00:00    40.816    -73.902   1.9   9.8  0.2  0.021  None\n",
      "11  12  2023-05-06 00:00    40.816    -73.902   7.3  38.8  0.4  0.002  None\n",
      "12  13  2023-05-09 00:00    40.816    -73.902   7.4   8.0  0.2  0.042  None\n",
      "13  14  2023-05-12 00:00    40.816    -73.902  20.8  19.7  0.7  0.035  None\n",
      "14  15  2023-05-15 00:00    40.816    -73.902   4.1   5.2  0.1  0.032  None\n",
      "15  16  2023-05-18 00:00    40.816    -73.902   2.7   4.9  0.1  0.037  None\n",
      "16  17  2023-05-21 00:00    40.816    -73.902   5.1  12.3  0.1  0.020  None\n",
      "17  18  2023-05-24 00:00    40.816    -73.902   4.7  15.4  0.3  0.023  None\n",
      "18  19  2023-05-27 00:00    40.816    -73.902   6.1  22.3  0.6  0.028  None\n",
      "19  20  2023-05-30 00:00    40.816    -73.902   6.5   5.6  0.1  0.032  None\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Downloads\\phase1_data (3).db\")\n",
    "df = pd.read_sql(\"SELECT * FROM ground_data LIMIT 20;\", conn)\n",
    "conn.close()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c37b9c",
   "metadata": {},
   "source": [
    "### Interpolation of missing NaN values\n",
    "\n",
    "Choosen method for interpolation includes:\n",
    "Handling short gaps with linear interpolation polation + rolling mean, big gaps with forward fill, which imputates on colums like c02, no2, c0 and o3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original % missing (sample):\n",
      "o3    0.0906\n",
      "dtype: float64\n",
      "\n",
      "Final % missing (should be 0 for all):\n",
      "Series([], dtype: float64)\n",
      "\n",
      "âœ… All imputations applied and saved to: ground_data_all_imputed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23060\\2875817492.py:84: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "# ----------------- USER PARAMETERS -----------------\n",
    "DB_PATH = r\"C:\\Users\\HP\\Downloads\\phase1_data (2).db\"   # adjust if needed\n",
    "TABLE = \"ground_data\"                                   # or \"weather_data\"\n",
    "TS_COL = \"timestamp\"\n",
    "OUT_CSV = \"ground_data_all_imputed.csv\"\n",
    "short_gap_max = 3        # <= this many consecutive missing steps -> treat as short gap (interpolate)\n",
    "rolling_window = 3       # smoothing window (centered rolling mean)\n",
    "WRITE_BACK_TO_DB = False # set True to write cleaned table as \"<TABLE>_imputed\" back to DB\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# --- load ---\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "df = pd.read_sql(f\"SELECT * FROM {TABLE}\", conn)\n",
    "conn.close()\n",
    "\n",
    "# --- timestamp parsing & drop unparsable ---\n",
    "df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[TS_COL]).copy()\n",
    "\n",
    "# --- separate columns ---\n",
    "all_cols = list(df.columns)\n",
    "non_ts_cols = [c for c in all_cols if c != TS_COL]\n",
    "\n",
    "# --- aggregate duplicate timestamps (mean for numeric cols, first() for others) ---\n",
    "# numeric aggregation\n",
    "numeric_cols = [c for c in non_ts_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "non_numeric_cols = [c for c in non_ts_cols if c not in numeric_cols]\n",
    "\n",
    "# numeric mean\n",
    "df_num = df.groupby(TS_COL, as_index=False)[numeric_cols].mean() if numeric_cols else pd.DataFrame({TS_COL: sorted(df[TS_COL].unique())})\n",
    "\n",
    "# keep first non-numeric if present\n",
    "if non_numeric_cols:\n",
    "    df_first = df.groupby(TS_COL, as_index=False)[non_numeric_cols].first()\n",
    "    df_agg = pd.merge(df_num, df_first, on=TS_COL, how=\"left\")\n",
    "else:\n",
    "    df_agg = df_num\n",
    "\n",
    "# --- sort & infer frequency ---\n",
    "df_agg = df_agg.sort_values(TS_COL).reset_index(drop=True)\n",
    "\n",
    "if df_agg[TS_COL].shape[0] >= 2:\n",
    "    med_delta = df_agg[TS_COL].diff().dropna().median()\n",
    "    try:\n",
    "        freq = to_offset(med_delta)\n",
    "    except Exception:\n",
    "        freq = \"H\"\n",
    "else:\n",
    "    freq = \"H\"\n",
    "\n",
    "# --- reindex to regular timeline ---\n",
    "full_idx = pd.date_range(start=df_agg[TS_COL].min(), end=df_agg[TS_COL].max(), freq=freq)\n",
    "df_reidx = df_agg.set_index(TS_COL).reindex(full_idx).rename_axis(TS_COL)\n",
    "\n",
    "# --- record original missing (for diagnostics) ---\n",
    "orig_missing_pct = (df_reidx.isna().sum() / len(df_reidx) * 100).round(4)\n",
    "\n",
    "# --- Impute numeric columns ---\n",
    "for col in numeric_cols:\n",
    "    if col not in df_reidx.columns:\n",
    "        continue\n",
    "    s = pd.to_numeric(df_reidx[col], errors=\"coerce\")\n",
    "\n",
    "    # If ENTIRE column is NaN -> fill with median (or 0 fallback)\n",
    "    if s.isna().all():\n",
    "        med = s.median()\n",
    "        if pd.isna(med):\n",
    "            med = 0.0\n",
    "        df_reidx[col] = med\n",
    "        print(f\"âš ï¸ Column '{col}' was completely empty â€” filled with median/fallback {med}\")\n",
    "        continue\n",
    "\n",
    "    # 1) time interpolation limited to short gaps\n",
    "    s_interp = s.interpolate(method=\"time\", limit=short_gap_max, limit_direction=\"both\")\n",
    "\n",
    "    # 2) smoothing of interpolated values\n",
    "    s_smooth = s_interp.rolling(window=rolling_window, min_periods=1, center=True).mean()\n",
    "\n",
    "    # 3) fill long gaps with forward-fill, then backward-fill for leading NaNs\n",
    "    s_filled = s_smooth.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "    # 4) if anything still NaN (rare), fill with column median or 0\n",
    "    if s_filled.isna().any():\n",
    "        fallback = s_filled.median()\n",
    "        if pd.isna(fallback):\n",
    "            fallback = 0.0\n",
    "        s_filled = s_filled.fillna(fallback)\n",
    "\n",
    "    df_reidx[col] = s_filled\n",
    "\n",
    "# --- Impute non-numeric columns ---\n",
    "for col in non_numeric_cols:\n",
    "    if col not in df_reidx.columns:\n",
    "        continue\n",
    "    s = df_reidx[col]\n",
    "\n",
    "    if s.isna().all():\n",
    "        df_reidx[col] = \"missing\"\n",
    "        print(f\"âš ï¸ Column '{col}' was completely empty â€” filled with 'missing'\")\n",
    "        continue\n",
    "\n",
    "    # fill with mode (most frequent) if possible\n",
    "    try:\n",
    "        mode_val = s.mode(dropna=True)[0]\n",
    "        df_reidx[col] = s.fillna(mode_val)\n",
    "    except Exception:\n",
    "        df_reidx[col] = s.fillna(\"missing\")\n",
    "\n",
    "# --- Final sanity: any remaining NaNs -> numeric 0, non-numeric 'missing' ---\n",
    "for col in df_reidx.columns:\n",
    "    if df_reidx[col].isna().any():\n",
    "        if pd.api.types.is_numeric_dtype(df_reidx[col]):\n",
    "            df_reidx[col] = df_reidx[col].fillna(0)\n",
    "        else:\n",
    "            df_reidx[col] = df_reidx[col].fillna(\"missing\")\n",
    "\n",
    "# --- Diagnostics ---\n",
    "final_missing_pct = (df_reidx.isna().sum() / len(df_reidx) * 100).round(6)   # should be zero now\n",
    "print(\"\\nOriginal % missing (sample):\")\n",
    "print(orig_missing_pct[orig_missing_pct > 0].sort_values(ascending=False).head(20))\n",
    "print(\"\\nFinal % missing (should be 0 for all):\")\n",
    "print(final_missing_pct[final_missing_pct > 0])\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_out = df_reidx.reset_index()\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nâœ… All imputations applied and saved to: {OUT_CSV}\")\n",
    "\n",
    "# --- Optional: write back to DB as new table ---\n",
    "if WRITE_BACK_TO_DB:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    new_table_name = f\"{TABLE}_imputed\"\n",
    "    df_out.to_sql(new_table_name, conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()\n",
    "    print(f\"âœ… Cleaned table written back to DB as: {new_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e328d",
   "metadata": {},
   "source": [
    "### Checking data after imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Percentage of NULL values in each column:\n",
      "id             0.0\n",
      "timestamp      0.0\n",
      "temperature    0.0\n",
      "wind_speed     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Downloads\\phase1_data (3).db\")\n",
    "\n",
    "# Load weather_data into pandas DataFrame\n",
    "df = pd.read_sql(\"SELECT * FROM weather_data\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Calculate percentage of NULL values in each column\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"ðŸ“Š Percentage of NULL values in each column:\")\n",
    "print(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84d658",
   "metadata": {},
   "source": [
    "### making csv files for model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gdf.csv has been imported into gdf.db as table 'gdf_table'.\n",
      "             timestamp           id   latitude  longitude      pm25  \\\n",
      "0  2023-04-01 00:00:00  5649.333333  40.757242 -73.997764  9.233333   \n",
      "1  2023-04-01 01:00:00  5650.333333  40.757242 -73.997764  9.100000   \n",
      "2  2023-04-01 02:00:00  5651.333333  40.757242 -73.997764  8.811111   \n",
      "3  2023-04-01 03:00:00  5652.333333  40.757242 -73.997764  9.133333   \n",
      "4  2023-04-01 04:00:00  5653.333333  40.757242 -73.997764  9.650000   \n",
      "\n",
      "         no2       so2      o3        co  \n",
      "0  21.937500  0.566667  0.0270  0.336500  \n",
      "1  20.191667  0.544444  0.0240  0.323000  \n",
      "2  18.016667  0.488889  0.0300  0.297778  \n",
      "3  17.733333  0.488889  0.0285  0.296333  \n",
      "4  20.083333  0.477778  0.0280  0.308889  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned CSV\n",
    "df = pd.read_csv(\"gdf.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Connect (this will create db file if it doesnâ€™t exist)\n",
    "conn = sqlite3.connect(\"gdf.db\")   # you can name it anything you want\n",
    "\n",
    "# Write DataFrame into database as a new table\n",
    "df.to_sql(\"gdf_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Verify\n",
    "print(\"âœ… gdf.csv has been imported into gdf.db as table 'gdf_table'.\")\n",
    "\n",
    "# Optional: show first 5 rows from DB\n",
    "print(pd.read_sql(\"SELECT * FROM gdf_table LIMIT 5\", conn))\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0beda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Percentage of NULL values in each column:\n",
      "timestamp    0.0\n",
      "id           0.0\n",
      "latitude     0.0\n",
      "longitude    0.0\n",
      "pm25         0.0\n",
      "no2          0.0\n",
      "so2          0.0\n",
      "o3           0.0\n",
      "co           0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(r'C:\\Users\\HP\\Desktop\\suraj\\gdf.db')\n",
    "\n",
    "# Load weather_data into pandas DataFrame\n",
    "df = pd.read_sql(\"SELECT * FROM gdf_table \", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Calculate percentage of NULL values in each column\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"ðŸ“Š Percentage of NULL values in each column:\")\n",
    "print(null_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7cb7e",
   "metadata": {},
   "source": [
    "## Putting data in database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0af83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported weather_data table from phase1_data3.db to wdf.csv\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your database\n",
    "DB_PATH = r'C:\\Users\\HP\\Downloads\\phase1_data (3).db'\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Load weather_data into a DataFrame\n",
    "df_weather = pd.read_sql(\"SELECT * FROM weather_data\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Save as CSV\n",
    "df_weather.to_csv(\"wdf.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Exported weather_data table from phase1_data3.db to wdf.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bdf8a",
   "metadata": {},
   "source": [
    "## Preprocessing ground weather data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96276c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… wdf.csv has been imported into gdf.db as table 'wdf_table'.\n",
      "   id            timestamp  temperature  wind_speed\n",
      "0   1  2023-04-01 00:00:00         16.6        22.7\n",
      "1   2  2023-04-02 00:00:00          9.9        27.7\n",
      "2   3  2023-04-03 00:00:00          9.4        12.6\n",
      "3   4  2023-04-04 00:00:00         15.0         6.5\n",
      "4   5  2023-04-05 00:00:00         13.2        12.6\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Load weather data CSV\n",
    "wdf = pd.read_csv(\"wdf.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Connect to existing gdf.db\n",
    "conn = sqlite3.connect(\"gdf.db\")\n",
    "\n",
    "# Write DataFrame into database as a new table\n",
    "wdf.to_sql(\"wdf_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"âœ… wdf.csv has been imported into gdf.db as table 'wdf_table'.\")\n",
    "\n",
    "# Optional: check first few rows\n",
    "print(pd.read_sql(\"SELECT * FROM wdf_table LIMIT 5\", conn))\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cf233e",
   "metadata": {},
   "source": [
    "### Merging satellite data with ground data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f58f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: merged_hourly_ground_with_daily_weather.csv\n",
      "Wrote merged table to DB as 'merged_hourly_ground_weather'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"gdf.db\"             # your DB file\n",
    "G_TABLE = \"gdf_table\"     # hourly ground table\n",
    "W_TABLE = \"wdf_table\"     # daily weather table\n",
    "\n",
    "# load\n",
    "conn = sqlite3.connect(DB)\n",
    "gdf = pd.read_sql(f\"SELECT * FROM {G_TABLE}\", conn)\n",
    "wdf = pd.read_sql(f\"SELECT * FROM {W_TABLE}\", conn)\n",
    "conn.close()\n",
    "\n",
    "# parse timestamps robustly\n",
    "gdf['timestamp'] = pd.to_datetime(gdf['timestamp'], errors='coerce')\n",
    "wdf['timestamp'] = pd.to_datetime(wdf['timestamp'], errors='coerce')\n",
    "\n",
    "# drop rows with bad timestamps\n",
    "gdf = gdf.dropna(subset=['timestamp']).copy()\n",
    "wdf = wdf.dropna(subset=['timestamp']).copy()\n",
    "\n",
    "# create a calendar date column (YYYY-MM-DD) for merging\n",
    "gdf['date'] = gdf['timestamp'].dt.date\n",
    "wdf['date'] = wdf['timestamp'].dt.date\n",
    "\n",
    "# if weather table contains multiple rows per date, aggregate it (mean for numeric)\n",
    "num_cols = wdf.select_dtypes(include='number').columns.tolist()\n",
    "if len(wdf.groupby('date')) > len(wdf):\n",
    "    aggs = {c: 'mean' for c in num_cols}\n",
    "    wdf_daily = wdf.groupby('date', as_index=False).agg(aggs)\n",
    "    # keep non-numeric columns if necessary (first)\n",
    "    for c in wdf.columns:\n",
    "        if c not in num_cols + ['timestamp', 'date']:\n",
    "            wdf_daily[c] = wdf.groupby('date')[c].first().values\n",
    "else:\n",
    "    # reduce to numeric + date\n",
    "    wdf_daily = wdf.drop(columns=[c for c in wdf.columns if c not in num_cols + ['date']], errors='ignore')\n",
    "\n",
    "# merge: broadcast daily weather to each hourly ground row with same date\n",
    "merged_hourly = pd.merge(gdf, wdf_daily, on='date', how='left', suffixes=('_ground', '_weather'))\n",
    "\n",
    "# optional: drop the helper 'date' column (or keep it)\n",
    "# merged_hourly = merged_hourly.drop(columns=['date'])\n",
    "\n",
    "# Save\n",
    "merged_hourly.to_csv(\"merged_hourly_ground_with_daily_weather.csv\", index=False)\n",
    "print(\"Saved: merged_hourly_ground_with_daily_weather.csv\")\n",
    "\n",
    "# Optional: write back to DB\n",
    "conn = sqlite3.connect(DB)\n",
    "merged_hourly.to_sql(\"merged_hourly_ground_weather\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "print(\"Wrote merged table to DB as 'merged_hourly_ground_weather'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c441f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect(r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\")\n",
    "df = pd.read_sql(\"SELECT * FROM ground_data LIMIT 20;\", conn)\n",
    "conn.close()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfb367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged CSV imported into gdf.db as table 'merged_table'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your merged CSV\n",
    "MERGED_CSV = \"merged_hourly_ground_with_daily_weather.csv\"   # or merged_daily_ground_weather.csv\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(MERGED_CSV, parse_dates=[\"timestamp\"], low_memory=False)\n",
    "\n",
    "# Connect to gdf.db\n",
    "conn = sqlite3.connect(\"gdf.db\")\n",
    "\n",
    "# Write as a new table inside gdf.db\n",
    "df.to_sql(\"merged_table\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Merged CSV imported into gdf.db as table 'merged_table'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dad8c",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING\n",
    "\n",
    "engineer_gdf_features.py\n",
    "\n",
    "Reads gdf.db -> merged_table and creates features:\n",
    "- hour, dayofweek, is_weekend\n",
    "- lag features for pollutant columns (grouped by latitude+longitude)\n",
    "- no2_wind = no2 * windspeed\n",
    "- pm25_temp = pm25 / temperature (safe for zero temps)\n",
    "- total_pollutants = no2 + so2 + co + pm25\n",
    "\n",
    "Outputs:\n",
    "- CSV: merged_table_engineered.csv\n",
    "- New DB table: merged_table_engineered\n",
    "\n",
    "Edit DB_PATH and other config below if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table: merged_table\n",
      "Creating lag features grouped by: ['latitude', 'longitude']\n",
      "Saving CSV to: merged_table_engineered.csv\n",
      "Writing engineered table to DB as: merged_table_engineered\n",
      "Done. Preview of engineered columns:\n",
      "                                    0                    1  \\\n",
      "timestamp         2023-04-06 00:00:00  2023-04-12 00:00:00   \n",
      "id_ground                 1077.083333          1102.416667   \n",
      "latitude                     40.75275             40.75275   \n",
      "longitude                  -73.973799           -73.973799   \n",
      "pm25                           10.725             9.177778   \n",
      "no2                         16.688889            13.188889   \n",
      "so2                             0.375             0.783333   \n",
      "o3                              0.028                0.047   \n",
      "co                           0.266578               0.1884   \n",
      "date                       2023-04-06           2023-04-12   \n",
      "id_weather                          6                   12   \n",
      "temperature                      15.0                 23.1   \n",
      "wind_speed                       15.8                 19.8   \n",
      "hour                                0                    0   \n",
      "dayofweek                           3                    2   \n",
      "is_weekend                          0                    0   \n",
      "total_pollutants            28.055467              23.3384   \n",
      "no2_wind                          NaN                  NaN   \n",
      "pm25_temp                       0.715             0.397306   \n",
      "no2_lag1                          NaN            16.688889   \n",
      "no2_lag3                          NaN                  NaN   \n",
      "no2_lag6                          NaN                  NaN   \n",
      "no2_lag24                         NaN                  NaN   \n",
      "no2_diff_lag1                     NaN                 -3.5   \n",
      "no2_roll_mean_3             16.688889            14.938889   \n",
      "no2_roll_mean_6             16.688889            14.938889   \n",
      "no2_roll_mean_24            16.688889            14.938889   \n",
      "pm25_lag1                         NaN               10.725   \n",
      "pm25_lag3                         NaN                  NaN   \n",
      "pm25_lag6                         NaN                  NaN   \n",
      "\n",
      "                                    2                    3  \\\n",
      "timestamp         2023-04-18 00:00:00  2023-04-24 00:00:00   \n",
      "id_ground                     1127.75          1153.166667   \n",
      "latitude                     40.75275             40.75275   \n",
      "longitude                  -73.973799           -73.973799   \n",
      "pm25                         2.919444             4.063889   \n",
      "no2                          5.851111             6.536111   \n",
      "so2                             0.325             0.222222   \n",
      "o3                           0.035667             0.032333   \n",
      "co                           0.183244             0.122511   \n",
      "date                       2023-04-18           2023-04-24   \n",
      "id_weather                         18                   24   \n",
      "temperature                      12.9                 12.7   \n",
      "wind_speed                       27.4                 13.7   \n",
      "hour                                0                    0   \n",
      "dayofweek                           1                    0   \n",
      "is_weekend                          0                    0   \n",
      "total_pollutants               9.2788            10.944733   \n",
      "no2_wind                          NaN                  NaN   \n",
      "pm25_temp                    0.226314             0.319991   \n",
      "no2_lag1                    13.188889             5.851111   \n",
      "no2_lag3                          NaN            16.688889   \n",
      "no2_lag6                          NaN                  NaN   \n",
      "no2_lag24                         NaN                  NaN   \n",
      "no2_diff_lag1               -7.337778                0.685   \n",
      "no2_roll_mean_3              11.90963              8.52537   \n",
      "no2_roll_mean_6              11.90963             10.56625   \n",
      "no2_roll_mean_24             11.90963             10.56625   \n",
      "pm25_lag1                    9.177778             2.919444   \n",
      "pm25_lag3                         NaN               10.725   \n",
      "pm25_lag6                         NaN                  NaN   \n",
      "\n",
      "                                    4  \n",
      "timestamp         2023-04-30 00:00:00  \n",
      "id_ground                      1178.5  \n",
      "latitude                     40.75275  \n",
      "longitude                  -73.973799  \n",
      "pm25                         3.322222  \n",
      "no2                          8.077778  \n",
      "so2                          0.277778  \n",
      "o3                               0.04  \n",
      "co                           0.297156  \n",
      "date                       2023-04-30  \n",
      "id_weather                         30  \n",
      "temperature                      12.4  \n",
      "wind_speed                       14.4  \n",
      "hour                                0  \n",
      "dayofweek                           6  \n",
      "is_weekend                          1  \n",
      "total_pollutants            11.974933  \n",
      "no2_wind                          NaN  \n",
      "pm25_temp                    0.267921  \n",
      "no2_lag1                     6.536111  \n",
      "no2_lag3                    13.188889  \n",
      "no2_lag6                          NaN  \n",
      "no2_lag24                         NaN  \n",
      "no2_diff_lag1                1.541667  \n",
      "no2_roll_mean_3              6.821667  \n",
      "no2_roll_mean_6             10.068556  \n",
      "no2_roll_mean_24            10.068556  \n",
      "pm25_lag1                    4.063889  \n",
      "pm25_lag3                    9.177778  \n",
      "pm25_lag6                         NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DB_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\"   # path to your gdf.db\n",
    "SRC_TABLE = \"merged_table\"\n",
    "OUT_CSV = r\"merged_table_engineered.csv\"\n",
    "OUT_TABLE = \"merged_table_engineered\"\n",
    "\n",
    "# Lags to generate (in rows). If data is hourly, 1 means previous hour (previous row).\n",
    "LAGS = [1, 3, 6, 24]\n",
    "\n",
    "# Which pollutant columns to create lag features for (will skip if column missing)\n",
    "LAG_COLS = [\"no2\", \"pm25\", \"so2\", \"co\", \"o3\", \"total_pollutants\"]\n",
    "\n",
    "# Rolling window sizes (in rows) to create rolling mean features\n",
    "ROLL_WINDOWS = [3, 6, 24]\n",
    "# ----------------------------\n",
    "\n",
    "def read_db_table(db_path, table_name):\n",
    "    if not os.path.isfile(db_path):\n",
    "        raise FileNotFoundError(f\"DB not found: {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(f'SELECT * FROM \"{table_name}\";', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def write_table_to_db(df, db_path, table_name, if_exists='replace'):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "    conn.close()\n",
    "\n",
    "def safe_numeric(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    print(\"Loading table:\", SRC_TABLE)\n",
    "    df = read_db_table(DB_PATH, SRC_TABLE)\n",
    "    if df.empty:\n",
    "        raise SystemExit(\"Source table is empty or not found.\")\n",
    "\n",
    "    # normalize column names (rename pm2.5 -> pm25)\n",
    "    if \"pm2.5\" in df.columns:\n",
    "        df = df.rename(columns={\"pm2.5\": \"pm25\"})\n",
    "    # also rename e.g. PM25 or pm25 variations to lower-case consistent names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # choose timestamp column: prefer 'timestamp' then 'date'\n",
    "    ts_col = None\n",
    "    for c in df.columns:\n",
    "        if c.lower() == \"timestamp\":\n",
    "            ts_col = c\n",
    "            break\n",
    "    if ts_col is None:\n",
    "        for c in df.columns:\n",
    "            if c.lower() == \"date\":\n",
    "                ts_col = c\n",
    "                break\n",
    "    if ts_col is None:\n",
    "        raise SystemExit(\"No timestamp/date column found (expected 'timestamp' or 'date').\")\n",
    "\n",
    "    # parse timestamp\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col], errors=\"coerce\")\n",
    "    if df[ts_col].isna().all():\n",
    "        raise SystemExit(f\"All values in {ts_col} failed to parse as datetime.\")\n",
    "\n",
    "    # create datetime features\n",
    "    df = df.sort_values(by=[ts_col]).reset_index(drop=True)\n",
    "    df[\"hour\"] = df[ts_col].dt.hour\n",
    "    df[\"dayofweek\"] = df[ts_col].dt.dayofweek\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5,6]).astype(int)  # 5=Sat, 6=Sun\n",
    "\n",
    "    # ensure numeric for key columns\n",
    "    numeric_candidates = [\"no2\", \"so2\", \"co\", \"pm25\", \"o3\", \"temperature\", \"windspeed\"]\n",
    "    df = safe_numeric(df, numeric_candidates)\n",
    "\n",
    "    # create total_pollutants = NO2 + SO2 + CO + PM25 (only sums available cols)\n",
    "    pollutants_for_sum = [c for c in [\"no2\",\"so2\",\"co\",\"pm25\"] if c in df.columns]\n",
    "    if pollutants_for_sum:\n",
    "        df[\"total_pollutants\"] = df[pollutants_for_sum].sum(axis=1, skipna=True)\n",
    "    else:\n",
    "        df[\"total_pollutants\"] = np.nan\n",
    "\n",
    "    # interaction features\n",
    "    if \"no2\" in df.columns and \"windspeed\" in df.columns:\n",
    "        df[\"no2_wind\"] = df[\"no2\"] * df[\"windspeed\"]\n",
    "    else:\n",
    "        df[\"no2_wind\"] = np.nan\n",
    "\n",
    "    # pm25_temp = pm25 / temperature (avoid division by zero)\n",
    "    if \"pm25\" in df.columns and \"temperature\" in df.columns:\n",
    "        temp = df[\"temperature\"].replace({0: np.nan})  # avoid div by zero\n",
    "        df[\"pm25_temp\"] = df[\"pm25\"] / temp\n",
    "    else:\n",
    "        df[\"pm25_temp\"] = np.nan\n",
    "\n",
    "    # prepare for lagging: sort by location (if present) and timestamp\n",
    "    group_cols = []\n",
    "    if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "        group_cols = [\"latitude\", \"longitude\"]\n",
    "        sort_cols = group_cols + [ts_col]\n",
    "    else:\n",
    "        sort_cols = [ts_col]\n",
    "\n",
    "    df = df.sort_values(by=sort_cols).reset_index(drop=True)\n",
    "\n",
    "    # create lag features grouped by location if available, else global\n",
    "    print(\"Creating lag features grouped by:\", group_cols if group_cols else \"GLOBAL\")\n",
    "    for col in LAG_COLS:\n",
    "        if col not in df.columns:\n",
    "            print(f\" - skipping lag for missing column: {col}\")\n",
    "            continue\n",
    "        # ensure numeric\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        for lag in LAGS:\n",
    "            lag_name = f\"{col}_lag{lag}\"\n",
    "            if group_cols:\n",
    "                df[lag_name] = df.groupby(group_cols)[col].shift(lag)\n",
    "            else:\n",
    "                df[lag_name] = df[col].shift(lag)\n",
    "        # difference to previous\n",
    "        diff_name = f\"{col}_diff_lag1\"\n",
    "        if group_cols:\n",
    "            df[diff_name] = df[col] - df.groupby(group_cols)[col].shift(1)\n",
    "        else:\n",
    "            df[diff_name] = df[col] - df[col].shift(1)\n",
    "\n",
    "        # rolling means (transform keeps index alignment)\n",
    "        for w in ROLL_WINDOWS:\n",
    "            roll_name = f\"{col}_roll_mean_{w}\"\n",
    "            if group_cols:\n",
    "                df[roll_name] = df.groupby(group_cols)[col].transform(lambda s: s.rolling(window=w, min_periods=1).mean())\n",
    "            else:\n",
    "                df[roll_name] = df[col].rolling(window=w, min_periods=1).mean()\n",
    "\n",
    "    # optional: keep NaNs for first few rows per group (they indicate lack of history)\n",
    "    # You can fill them later for modeling: e.g., df.fillna(method='ffill') or df.fillna(0)\n",
    "\n",
    "    # Save outputs\n",
    "    print(\"Saving CSV to:\", OUT_CSV)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "    # Write engineered table back to DB (new table, do not overwrite original)\n",
    "    print(\"Writing engineered table to DB as:\", OUT_TABLE)\n",
    "    write_table_to_db(df, DB_PATH, OUT_TABLE, if_exists='replace')\n",
    "\n",
    "    # Quick preview\n",
    "    print(\"Done. Preview of engineered columns:\")\n",
    "    print(df.head(5).T.head(30))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table: merged_table_engineered  |  Total rows: 2184\n",
      "\n",
      "column                                      nulls  empties  total_empty     %null    %total\n",
      "----------------------------------------------------------------------------------------------------\n",
      "no2_wind                                     2184        0         2184    100.0%    100.0%\n",
      "no2_lag24                                      79        0           79 3.617216% 3.617216%\n",
      "pm25_lag24                                     79        0           79 3.617216% 3.617216%\n",
      "so2_lag24                                      79        0           79 3.617216% 3.617216%\n",
      "co_lag24                                       79        0           79 3.617216% 3.617216%\n",
      "o3_lag24                                       79        0           79 3.617216% 3.617216%\n",
      "total_pollutants_lag24                         79        0           79 3.617216% 3.617216%\n",
      "no2_lag6                                       25        0           25 1.144689% 1.144689%\n",
      "pm25_lag6                                      25        0           25 1.144689% 1.144689%\n",
      "so2_lag6                                       25        0           25 1.144689% 1.144689%\n",
      "co_lag6                                        25        0           25 1.144689% 1.144689%\n",
      "o3_lag6                                        25        0           25 1.144689% 1.144689%\n",
      "total_pollutants_lag6                          25        0           25 1.144689% 1.144689%\n",
      "no2_lag3                                       13        0           13 0.595238% 0.595238%\n",
      "pm25_lag3                                      13        0           13 0.595238% 0.595238%\n",
      "so2_lag3                                       13        0           13 0.595238% 0.595238%\n",
      "co_lag3                                        13        0           13 0.595238% 0.595238%\n",
      "o3_lag3                                        13        0           13 0.595238% 0.595238%\n",
      "total_pollutants_lag3                          13        0           13 0.595238% 0.595238%\n",
      "no2_lag1                                        5        0            5 0.228938% 0.228938%\n",
      "no2_diff_lag1                                   5        0            5 0.228938% 0.228938%\n",
      "pm25_lag1                                       5        0            5 0.228938% 0.228938%\n",
      "pm25_diff_lag1                                  5        0            5 0.228938% 0.228938%\n",
      "so2_lag1                                        5        0            5 0.228938% 0.228938%\n",
      "so2_diff_lag1                                   5        0            5 0.228938% 0.228938%\n",
      "co_lag1                                         5        0            5 0.228938% 0.228938%\n",
      "co_diff_lag1                                    5        0            5 0.228938% 0.228938%\n",
      "o3_lag1                                         5        0            5 0.228938% 0.228938%\n",
      "o3_diff_lag1                                    5        0            5 0.228938% 0.228938%\n",
      "total_pollutants_lag1                           5        0            5 0.228938% 0.228938%\n",
      "total_pollutants_diff_lag1                      5        0            5 0.228938% 0.228938%\n",
      "timestamp                                       0        0            0      0.0%      0.0%\n",
      "id_ground                                       0        0            0      0.0%      0.0%\n",
      "latitude                                        0        0            0      0.0%      0.0%\n",
      "longitude                                       0        0            0      0.0%      0.0%\n",
      "pm25                                            0        0            0      0.0%      0.0%\n",
      "no2                                             0        0            0      0.0%      0.0%\n",
      "so2                                             0        0            0      0.0%      0.0%\n",
      "o3                                              0        0            0      0.0%      0.0%\n",
      "co                                              0        0            0      0.0%      0.0%\n",
      "date                                            0        0            0      0.0%      0.0%\n",
      "id_weather                                      0        0            0      0.0%      0.0%\n",
      "temperature                                     0        0            0      0.0%      0.0%\n",
      "wind_speed                                      0        0            0      0.0%      0.0%\n",
      "hour                                            0        0            0      0.0%      0.0%\n",
      "dayofweek                                       0        0            0      0.0%      0.0%\n",
      "is_weekend                                      0        0            0      0.0%      0.0%\n",
      "total_pollutants                                0        0            0      0.0%      0.0%\n",
      "pm25_temp                                       0        0            0      0.0%      0.0%\n",
      "no2_roll_mean_3                                 0        0            0      0.0%      0.0%\n",
      "no2_roll_mean_6                                 0        0            0      0.0%      0.0%\n",
      "no2_roll_mean_24                                0        0            0      0.0%      0.0%\n",
      "pm25_roll_mean_3                                0        0            0      0.0%      0.0%\n",
      "pm25_roll_mean_6                                0        0            0      0.0%      0.0%\n",
      "pm25_roll_mean_24                               0        0            0      0.0%      0.0%\n",
      "so2_roll_mean_3                                 0        0            0      0.0%      0.0%\n",
      "so2_roll_mean_6                                 0        0            0      0.0%      0.0%\n",
      "so2_roll_mean_24                                0        0            0      0.0%      0.0%\n",
      "co_roll_mean_3                                  0        0            0      0.0%      0.0%\n",
      "co_roll_mean_6                                  0        0            0      0.0%      0.0%\n",
      "co_roll_mean_24                                 0        0            0      0.0%      0.0%\n",
      "o3_roll_mean_3                                  0        0            0      0.0%      0.0%\n",
      "o3_roll_mean_6                                  0        0            0      0.0%      0.0%\n",
      "o3_roll_mean_24                                 0        0            0      0.0%      0.0%\n",
      "total_pollutants_roll_mean_3                    0        0            0      0.0%      0.0%\n",
      "total_pollutants_roll_mean_6                    0        0            0      0.0%      0.0%\n",
      "total_pollutants_roll_mean_24                   0        0            0      0.0%      0.0%\n",
      "\n",
      "Showing up to 10 sample rows that contain any NULL or empty-string value:\n",
      "\n",
      "timestamp | id_ground | latitude | longitude | pm25 | no2 | so2 | o3 | co | date | id_weather | temperature | wind_speed | hour | dayofweek | is_weekend | total_pollutants | no2_wind | pm25_temp | no2_lag1 | no2_lag3 | no2_lag6 | no2_lag24 | no2_diff_lag1 | no2_roll_mean_3 | no2_roll_mean_6 | no2_roll_mean_24 | pm25_lag1 | pm25_lag3 | pm25_lag6 | pm25_lag24 | pm25_diff_lag1 | pm25_roll_mean_3 | pm25_roll_mean_6 | pm25_roll_mean_24 | so2_lag1 | so2_lag3 | so2_lag6 | so2_lag24 | so2_diff_lag1 | so2_roll_mean_3 | so2_roll_mean_6 | so2_roll_mean_24 | co_lag1 | co_lag3 | co_lag6 | co_lag24 | co_diff_lag1 | co_roll_mean_3 | co_roll_mean_6 | co_roll_mean_24 | o3_lag1 | o3_lag3 | o3_lag6 | o3_lag24 | o3_diff_lag1 | o3_roll_mean_3 | o3_roll_mean_6 | o3_roll_mean_24 | total_pollutants_lag1 | total_pollutants_lag3 | total_pollutants_lag6 | total_pollutants_lag24 | total_pollutants_diff_lag1 | total_pollutants_roll_mean_3 | total_pollutants_roll_mean_6 | total_pollutants_roll_mean_24\n",
      "2023-04-06 00:00:00 | 1077.0833333333333 | 40.75274958333333 | -73.97379916666667 | 10.725 | 16.688888888888894 | 0.3750000000000001 | 0.0279999999999999 | 0.2665777777777777 | 2023-04-06 | 6 | 15.0 | 15.8 | 0 | 3 | 0 | 28.055466666666668 | NULL | 0.715 | NULL | NULL | NULL | NULL | NULL | 16.688888888888894 | 16.688888888888894 | 16.688888888888894 | NULL | NULL | NULL | NULL | NULL | 10.725 | 10.725 | 10.725 | NULL | NULL | NULL | NULL | NULL | 0.3750000000000001 | 0.3750000000000001 | 0.3750000000000001 | NULL | NULL | NULL | NULL | NULL | 0.2665777777777777 | 0.2665777777777777 | 0.2665777777777777 | NULL | NULL | NULL | NULL | NULL | 0.0279999999999999 | 0.0279999999999999 | 0.0279999999999999 | NULL | NULL | NULL | NULL | NULL | 28.055466666666668 | 28.055466666666668 | 28.055466666666668\n",
      "2023-04-12 00:00:00 | 1102.4166666666667 | 40.75274958333333 | -73.97379916666667 | 9.177777777777775 | 13.188888888888895 | 0.7833333333333333 | 0.047 | 0.1884 | 2023-04-12 | 12 | 23.1 | 19.8 | 0 | 2 | 0 | 23.338400000000004 | NULL | 0.39730639730639716 | 16.688888888888894 | NULL | NULL | NULL | -3.4999999999999982 | 14.938888888888894 | 14.938888888888894 | 14.938888888888894 | 10.725 | NULL | NULL | NULL | -1.5472222222222243 | 9.951388888888888 | 9.951388888888888 | 9.951388888888888 | 0.3750000000000001 | NULL | NULL | NULL | 0.4083333333333332 | 0.5791666666666667 | 0.5791666666666667 | 0.5791666666666667 | 0.2665777777777777 | NULL | NULL | NULL | -0.07817777777777768 | 0.22748888888888885 | 0.22748888888888885 | 0.22748888888888885 | 0.0279999999999999 | NULL | NULL | NULL | 0.0190000000000001 | 0.03749999999999995 | 0.03749999999999995 | 0.03749999999999995 | 28.055466666666668 | NULL | NULL | NULL | -4.717066666666664 | 25.696933333333334 | 25.696933333333334 | 25.696933333333334\n",
      "2023-04-18 00:00:00 | 1127.75 | 40.75274958333333 | -73.97379916666667 | 2.9194444444444465 | 5.851111111111123 | 0.3250000000000001 | 0.0356666666666666 | 0.1832444444444444 | 2023-04-18 | 18 | 12.9 | 27.4 | 0 | 1 | 0 | 9.278800000000015 | NULL | 0.22631352282515088 | 13.188888888888895 | NULL | NULL | NULL | -7.337777777777772 | 11.909629629629636 | 11.909629629629636 | 11.909629629629636 | 9.177777777777775 | NULL | NULL | NULL | -6.258333333333329 | 7.607407407407408 | 7.607407407407408 | 7.607407407407408 | 0.7833333333333333 | NULL | NULL | NULL | -0.4583333333333332 | 0.4944444444444445 | 0.4944444444444445 | 0.4944444444444445 | 0.1884 | NULL | NULL | NULL | -0.005155555555555608 | 0.2127407407407407 | 0.2127407407407407 | 0.2127407407407407 | 0.047 | NULL | NULL | NULL | -0.011333333333333404 | 0.03688888888888883 | 0.03688888888888883 | 0.03688888888888883 | 23.338400000000004 | NULL | NULL | NULL | -14.059599999999989 | 20.224222222222227 | 20.224222222222227 | 20.224222222222227\n",
      "2023-04-24 00:00:00 | 1153.1666666666667 | 40.75274958333333 | -73.97379916666667 | 4.063888888888891 | 6.536111111111126 | 0.2222222222222223 | 0.0323333333333333 | 0.1225111111111111 | 2023-04-24 | 24 | 12.7 | 13.7 | 0 | 0 | 0 | 10.94473333333335 | NULL | 0.3199912510936135 | 5.851111111111123 | 16.688888888888894 | NULL | NULL | 0.6850000000000023 | 8.525370370370382 | 10.566250000000009 | 10.566250000000009 | 2.9194444444444465 | 10.725 | NULL | NULL | 1.1444444444444444 | 5.387037037037037 | 6.721527777777778 | 6.721527777777778 | 0.3250000000000001 | 0.3750000000000001 | NULL | NULL | -0.10277777777777783 | 0.4435185185185186 | 0.426388888888889 | 0.426388888888889 | 0.1832444444444444 | 0.2665777777777777 | NULL | NULL | -0.060733333333333306 | 0.16471851851851851 | 0.19018333333333332 | 0.19018333333333332 | 0.0356666666666666 | 0.0279999999999999 | NULL | NULL | -0.0033333333333332993 | 0.038333333333333296 | 0.03574999999999995 | 0.03574999999999995 | 9.278800000000015 | 28.055466666666668 | NULL | NULL | 1.665933333333335 | 14.520644444444457 | 17.904350000000008 | 17.904350000000008\n",
      "2023-04-30 00:00:00 | 1178.5 | 40.75274958333333 | -73.97379916666667 | 3.322222222222223 | 8.077777777777793 | 0.2777777777777779 | 0.04 | 0.2971555555555556 | 2023-04-30 | 30 | 12.4 | 14.4 | 0 | 6 | 1 | 11.97493333333335 | NULL | 0.2679211469534051 | 6.536111111111126 | 13.188888888888895 | NULL | NULL | 1.5416666666666679 | 6.8216666666666805 | 10.068555555555566 | 10.068555555555566 | 4.063888888888891 | 9.177777777777775 | NULL | NULL | -0.741666666666668 | 3.435185185185187 | 6.041666666666667 | 6.041666666666667 | 0.2222222222222223 | 0.7833333333333333 | NULL | NULL | 0.05555555555555561 | 0.2750000000000001 | 0.3966666666666668 | 0.3966666666666668 | 0.1225111111111111 | 0.1884 | NULL | NULL | 0.17464444444444452 | 0.20097037037037038 | 0.21157777777777778 | 0.21157777777777778 | 0.0323333333333333 | 0.047 | NULL | NULL | 0.0076666666666667035 | 0.03599999999999997 | 0.03659999999999996 | 0.03659999999999996 | 10.94473333333335 | 23.338400000000004 | NULL | NULL | 1.0302000000000007 | 10.73282222222224 | 16.71846666666668 | 16.71846666666668\n",
      "2023-05-06 00:00:00 | 1456.3333333333333 | 40.75274958333333 | -73.97379916666667 | 7.916666666666668 | 31.894444444444463 | 0.4972222222222224 | 0.0063333333333333 | 0.4698222222222223 | 2023-05-06 | 36 | 16.1 | 11.5 | 0 | 5 | 1 | 40.77815555555557 | NULL | 0.4917184265010352 | 8.077777777777793 | 5.851111111111123 | NULL | NULL | 23.81666666666667 | 15.502777777777794 | 13.706203703703716 | 13.706203703703716 | 3.322222222222223 | 2.9194444444444465 | NULL | NULL | 4.594444444444445 | 5.100925925925927 | 6.354166666666667 | 6.354166666666667 | 0.2777777777777779 | 0.3250000000000001 | NULL | NULL | 0.2194444444444445 | 0.33240740740740754 | 0.41342592592592603 | 0.41342592592592603 | 0.2971555555555556 | 0.1832444444444444 | NULL | NULL | 0.1726666666666667 | 0.2964962962962963 | 0.2546185185185185 | 0.2546185185185185 | 0.04 | 0.0356666666666666 | NULL | NULL | -0.0336666666666667 | 0.0262222222222222 | 0.03155555555555552 | 0.03155555555555552 | 11.97493333333335 | 9.278800000000015 | NULL | NULL | 28.80322222222222 | 21.232607407407425 | 20.728414814814826 | 20.728414814814826\n",
      "2023-05-12 00:00:00 | 1237.5833333333333 | 40.75274958333333 | -73.97379916666667 | 21.063888888888894 | 18.72777777777779 | 0.8083333333333336 | 0.0353333333333333 | 0.3114444444444446 | 2023-05-12 | 42 | 24.3 | 13.7 | 0 | 4 | 0 | 40.911444444444456 | NULL | 0.8668267032464565 | 31.894444444444463 | 6.536111111111126 | 16.688888888888894 | NULL | -13.166666666666675 | 19.56666666666668 | 14.046018518518531 | 14.42357142857144 | 7.916666666666668 | 4.063888888888891 | 10.725 | NULL | 13.147222222222226 | 10.767592592592594 | 8.077314814814816 | 8.455555555555557 | 0.4972222222222224 | 0.2222222222222223 | 0.3750000000000001 | NULL | 0.31111111111111117 | 0.527777777777778 | 0.4856481481481483 | 0.46984126984126995 | 0.4698222222222223 | 0.1225111111111111 | 0.2665777777777777 | NULL | -0.15837777777777773 | 0.3594740740740742 | 0.2620962962962963 | 0.26273650793650793 | 0.0063333333333333 | 0.0323333333333333 | 0.0279999999999999 | NULL | 0.028999999999999998 | 0.027222222222222203 | 0.03277777777777775 | 0.03209523809523806 | 40.77815555555557 | 10.94473333333335 | 28.055466666666668 | NULL | 0.13328888888888457 | 31.221511111111127 | 22.87107777777779 | 23.611704761904775\n",
      "2023-05-18 00:00:00 | 1359.5 | 40.75274958333333 | -73.97379916666667 | 4.069444444444447 | 7.916666666666681 | 0.3694444444444446 | 0.0383333333333333 | 0.0930222222222223 | 2023-05-18 | 48 | 12.1 | 13.7 | 0 | 3 | 0 | 12.448577777777796 | NULL | 0.3363177226813593 | 18.72777777777779 | 8.077777777777793 | 13.188888888888895 | NULL | -10.811111111111106 | 19.512962962962977 | 13.167314814814828 | 13.610208333333345 | 21.063888888888894 | 3.322222222222223 | 9.177777777777775 | NULL | -16.994444444444447 | 11.016666666666671 | 7.225925925925929 | 7.907291666666668 | 0.8083333333333336 | 0.2777777777777779 | 0.7833333333333333 | NULL | -0.438888888888889 | 0.5583333333333336 | 0.4166666666666668 | 0.45729166666666676 | 0.3114444444444446 | 0.2971555555555556 | 0.1884 | NULL | -0.2184222222222223 | 0.29142962962962976 | 0.24620000000000006 | 0.24152222222222225 | 0.0353333333333333 | 0.04 | 0.047 | NULL | 0.0030000000000000027 | 0.026666666666666634 | 0.0313333333333333 | 0.03287499999999996 | 40.911444444444456 | 11.97493333333335 | 23.338400000000004 | NULL | -28.46286666666666 | 31.37939259259261 | 21.056107407407424 | 22.2163138888889\n",
      "2023-05-24 00:00:00 | 1288.25 | 40.75274958333333 | -73.97379916666667 | 7.194444444444447 | 21.6888888888889 | 0.1611111111111111 | 0.0223333333333333 | 0.2545333333333334 | 2023-05-24 | 54 | 17.8 | 11.2 | 0 | 2 | 0 | 29.298977777777793 | NULL | 0.4041822721598004 | 7.916666666666681 | 31.894444444444463 | 5.851111111111123 | NULL | 13.772222222222219 | 16.111111111111125 | 15.80694444444446 | 14.507839506172852 | 4.069444444444447 | 7.916666666666668 | 2.9194444444444465 | NULL | 3.125 | 10.77592592592593 | 7.9384259259259276 | 7.828086419753088 | 0.3694444444444446 | 0.4972222222222224 | 0.3250000000000001 | NULL | -0.20833333333333348 | 0.4462962962962964 | 0.389351851851852 | 0.4243827160493828 | 0.0930222222222223 | 0.4698222222222223 | 0.1832444444444444 | NULL | 0.1615111111111111 | 0.21966666666666676 | 0.2580814814814816 | 0.2429679012345679 | 0.0383333333333333 | 0.0063333333333333 | 0.0356666666666666 | NULL | -0.016000000000000004 | 0.031999999999999966 | 0.029111111111111084 | 0.031703703703703665 | 12.448577777777796 | 40.77815555555557 | 9.278800000000015 | NULL | 16.850399999999997 | 27.553000000000015 | 24.39280370370372 | 23.00327654320989\n",
      "2023-05-30 00:00:00 | 1613.5833333333333 | 40.75274958333333 | -73.97379916666667 | 8.744949494949497 | 7.552777777777792 | 0.4444444444444446 | 0.032 | 0.1752666666666668 | 2023-05-30 | 60 | 17.9 | 13.0 | 0 | 1 | 0 | 16.9174383838384 | NULL | 0.48854466452231826 | 21.6888888888889 | 18.72777777777779 | 6.536111111111126 | NULL | -14.13611111111111 | 12.386111111111125 | 15.976388888888904 | 13.812333333333346 | 7.194444444444447 | 21.063888888888894 | 4.063888888888891 | NULL | 1.5505050505050493 | 6.669612794612797 | 8.718602693602696 | 7.919772727272729 | 0.1611111111111111 | 0.8083333333333336 | 0.2222222222222223 | NULL | 0.2833333333333335 | 0.3250000000000001 | 0.426388888888889 | 0.42638888888888904 | 0.2545333333333334 | 0.3114444444444446 | 0.1225111111111111 | NULL | -0.0792666666666666 | 0.17427407407407416 | 0.26687407407407415 | 0.2361977777777778 | 0.0223333333333333 | 0.0353333333333333 | 0.0323333333333333 | NULL | 0.009666666666666702 | 0.03088888888888887 | 0.029055555555555532 | 0.0317333333333333 | 29.298977777777793 | 40.911444444444456 | 10.94473333333335 | NULL | -12.381539393939391 | 19.554997979798 | 25.38825454545456 | 22.39469272727274\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "show_merged_engineered_nulls.py\n",
    "\n",
    "Connects to gfd.db and displays NULL/empty-string counts + percentages\n",
    "for every column in the merged_engineered table. Also prints sample rows\n",
    "that contain any NULL/empty value.\n",
    "\n",
    "No CSV is written â€” output is displayed to the console only.\n",
    "\n",
    "Edit DB_PATH if your DB is in a different folder.\n",
    "\"\"\"\n",
    "import os\n",
    "import sqlite3\n",
    "import sys\n",
    "\n",
    "# === EDIT IF NEEDED ===\n",
    "DB_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\"                 # full path if needed, e.g. r\"C:\\Users\\HP\\Downloads\\gfd.db\"\n",
    "TABLE = \"merged_table_engineered\"\n",
    "SAMPLE_ROWS_TO_SHOW = 10\n",
    "# ======================\n",
    "\n",
    "def get_columns(conn, table):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info('{table}');\")\n",
    "    return [row[1] for row in cur.fetchall()]\n",
    "\n",
    "def total_rows(conn, table):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM '{table}';\")\n",
    "    return cur.fetchone()[0]\n",
    "\n",
    "def count_null_and_empty(conn, table, col):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM '{table}' WHERE \\\"{col}\\\" IS NULL;\")\n",
    "    null_count = cur.fetchone()[0]\n",
    "    cur.execute(f\"SELECT COUNT(*) FROM '{table}' WHERE TRIM(CAST(\\\"{col}\\\" AS TEXT)) = '' AND \\\"{col}\\\" IS NOT NULL;\")\n",
    "    empty_count = cur.fetchone()[0]\n",
    "    return null_count, empty_count\n",
    "\n",
    "def sample_rows_with_any_empty(conn, table, cols, limit=10):\n",
    "    cur = conn.cursor()\n",
    "    conds = []\n",
    "    for c in cols:\n",
    "        conds.append(f\"\\\"{c}\\\" IS NULL\")\n",
    "        conds.append(f\"TRIM(CAST(\\\"{c}\\\" AS TEXT)) = ''\")\n",
    "    where_clause = \" OR \".join(conds)\n",
    "    sql = f\"SELECT * FROM '{table}' WHERE {where_clause} LIMIT {limit};\"\n",
    "    cur.execute(sql)\n",
    "    rows = cur.fetchall()\n",
    "    headers = [d[0] for d in cur.description] if cur.description else []\n",
    "    return headers, rows\n",
    "\n",
    "def main():\n",
    "    if not os.path.isfile(DB_PATH):\n",
    "        print(\"ERROR: DB file not found at:\", DB_PATH)\n",
    "        sys.exit(1)\n",
    "\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cols = get_columns(conn, TABLE)\n",
    "    if not cols:\n",
    "        print(f\"ERROR: Table '{TABLE}' not found or has no columns in {DB_PATH}\")\n",
    "        conn.close()\n",
    "        sys.exit(1)\n",
    "\n",
    "    total = total_rows(conn, TABLE)\n",
    "    if total == 0:\n",
    "        print(f\"Table '{TABLE}' has 0 rows.\")\n",
    "        conn.close()\n",
    "        sys.exit(0)\n",
    "\n",
    "    report = []\n",
    "    for c in cols:\n",
    "        null_count, empty_count = count_null_and_empty(conn, TABLE, c)\n",
    "        total_empty = null_count + empty_count\n",
    "        pct_null = round(100.0 * null_count / total, 6)\n",
    "        pct_total_empty = round(100.0 * total_empty / total, 6)\n",
    "        report.append({\n",
    "            \"column\": c,\n",
    "            \"null_count\": null_count,\n",
    "            \"empty_string_count\": empty_count,\n",
    "            \"total_empty_count\": total_empty,\n",
    "            \"pct_null_%\": pct_null,\n",
    "            \"pct_total_empty_%\": pct_total_empty\n",
    "        })\n",
    "\n",
    "    # sort by pct_total_empty descending\n",
    "    report_sorted = sorted(report, key=lambda r: r[\"pct_total_empty_%\"], reverse=True)\n",
    "\n",
    "    # print summary\n",
    "    print(f\"\\nTable: {TABLE}  |  Total rows: {total}\\n\")\n",
    "    print(f\"{'column':40} {'nulls':>8} {'empties':>8} {'total_empty':>12} {'%null':>9} {'%total':>9}\")\n",
    "    print(\"-\" * 100)\n",
    "    for r in report_sorted:\n",
    "        print(f\"{r['column']:<40} {r['null_count']:>8} {r['empty_string_count']:>8} {r['total_empty_count']:>12} {r['pct_null_%']:>8}% {r['pct_total_empty_%']:>8}%\")\n",
    "\n",
    "    # show sample rows containing any empty value\n",
    "    print(f\"\\nShowing up to {SAMPLE_ROWS_TO_SHOW} sample rows that contain any NULL or empty-string value:\")\n",
    "    headers, rows = sample_rows_with_any_empty(conn, TABLE, cols, limit=SAMPLE_ROWS_TO_SHOW)\n",
    "    if rows:\n",
    "        # print header\n",
    "        print(\"\\n\" + \" | \".join(headers))\n",
    "        for r in rows:\n",
    "            # truncate long fields for console readability\n",
    "            print(\" | \".join((str(v)[:120] if v is not None else \"NULL\") for v in r))\n",
    "    else:\n",
    "        print(\"No sample rows with empties found (first\", SAMPLE_ROWS_TO_SHOW, \"rows).\")\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e1057",
   "metadata": {},
   "source": [
    "### impute_and_drop_no2_wind.py\n",
    "\n",
    "- Loads gfd.db -> merged_engineered\n",
    "- Drops column 'no2_wind' if present\n",
    "- Imputes numeric columns with median\n",
    "- Imputes object/string columns with mode (or '' if no mode)\n",
    "- Backs up original table, writes imputed table replacing merged_engineered\n",
    "- Saves a CSV copy merged_engineered_imputed.csv for inspection\n",
    "\n",
    "Edit DB_PATH/TABLE if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table: merged_table_engineered from DB: C:\\Users\\HP\\Desktop\\suraj\\gdf.db\n",
      "Original shape: (2184, 67)\n",
      "Dropped column: no2_wind\n",
      "Numeric columns: ['id_ground', 'latitude', 'longitude', 'pm25', 'no2', 'so2', 'o3', 'co', 'id_weather', 'temperature', 'wind_speed', 'hour', 'dayofweek', 'is_weekend', 'total_pollutants', 'pm25_temp', 'no2_lag1', 'no2_lag3', 'no2_lag6', 'no2_lag24', 'no2_diff_lag1', 'no2_roll_mean_3', 'no2_roll_mean_6', 'no2_roll_mean_24', 'pm25_lag1', 'pm25_lag3', 'pm25_lag6', 'pm25_lag24', 'pm25_diff_lag1', 'pm25_roll_mean_3', 'pm25_roll_mean_6', 'pm25_roll_mean_24', 'so2_lag1', 'so2_lag3', 'so2_lag6', 'so2_lag24', 'so2_diff_lag1', 'so2_roll_mean_3', 'so2_roll_mean_6', 'so2_roll_mean_24', 'co_lag1', 'co_lag3', 'co_lag6', 'co_lag24', 'co_diff_lag1', 'co_roll_mean_3', 'co_roll_mean_6', 'co_roll_mean_24', 'o3_lag1', 'o3_lag3', 'o3_lag6', 'o3_lag24', 'o3_diff_lag1', 'o3_roll_mean_3', 'o3_roll_mean_6', 'o3_roll_mean_24', 'total_pollutants_lag1', 'total_pollutants_lag3', 'total_pollutants_lag6', 'total_pollutants_lag24', 'total_pollutants_diff_lag1', 'total_pollutants_roll_mean_3', 'total_pollutants_roll_mean_6', 'total_pollutants_roll_mean_24']\n",
      "Datetime-like columns: ['timestamp', 'date']\n",
      "Object/categorical columns to impute with mode: []\n",
      "Imputed numeric column 'id_ground' with median = 6719.666666666667\n",
      "Imputed numeric column 'latitude' with median = 40.75724183333333\n",
      "Imputed numeric column 'longitude' with median = -73.99776433333334\n",
      "Imputed numeric column 'pm25' with median = 7.91388888888889\n",
      "Imputed numeric column 'no2' with median = 12.462500000000015\n",
      "Imputed numeric column 'so2' with median = 0.3666666666666667\n",
      "Imputed numeric column 'o3' with median = 0.034\n",
      "Imputed numeric column 'co' with median = 0.2920833333333333\n",
      "Imputed numeric column 'id_weather' with median = 46.0\n",
      "Imputed numeric column 'temperature' with median = 18.8\n",
      "Imputed numeric column 'wind_speed' with median = 13.7\n",
      "Imputed numeric column 'hour' with median = 11.5\n",
      "Imputed numeric column 'dayofweek' with median = 3.0\n",
      "Imputed numeric column 'is_weekend' with median = 0.0\n",
      "Imputed numeric column 'total_pollutants' with median = 22.165277777777796\n",
      "Imputed numeric column 'pm25_temp' with median = 0.446639858190679\n",
      "Imputed numeric column 'no2_lag1' with median = 12.46666666666668\n",
      "Imputed numeric column 'no2_lag3' with median = 12.46666666666668\n",
      "Imputed numeric column 'no2_lag6' with median = 12.45833333333335\n",
      "Imputed numeric column 'no2_lag24' with median = 12.383333333333349\n",
      "Imputed numeric column 'no2_diff_lag1' with median = 0.008333333333331971\n",
      "Imputed numeric column 'no2_roll_mean_3' with median = 12.522222222222236\n",
      "Imputed numeric column 'no2_roll_mean_6' with median = 12.639583333333348\n",
      "Imputed numeric column 'no2_roll_mean_24' with median = 13.338252314814827\n",
      "Imputed numeric column 'pm25_lag1' with median = 7.9055555555555586\n",
      "Imputed numeric column 'pm25_lag3' with median = 7.894444444444446\n",
      "Imputed numeric column 'pm25_lag6' with median = 7.883333333333337\n",
      "Imputed numeric column 'pm25_lag24' with median = 7.872222222222223\n",
      "Imputed numeric column 'pm25_diff_lag1' with median = 0.005555555555555536\n",
      "Imputed numeric column 'pm25_roll_mean_3' with median = 7.941543209876548\n",
      "Imputed numeric column 'pm25_roll_mean_6' with median = 7.9606481481481595\n",
      "Imputed numeric column 'pm25_roll_mean_24' with median = 8.260185185185186\n",
      "Imputed numeric column 'so2_lag1' with median = 0.3666666666666667\n",
      "Imputed numeric column 'so2_lag3' with median = 0.3666666666666667\n",
      "Imputed numeric column 'so2_lag6' with median = 0.3666666666666667\n",
      "Imputed numeric column 'so2_lag24' with median = 0.3666666666666667\n",
      "Imputed numeric column 'so2_diff_lag1' with median = 0.0\n",
      "Imputed numeric column 'so2_roll_mean_3' with median = 0.36666666666666675\n",
      "Imputed numeric column 'so2_roll_mean_6' with median = 0.3685185185185187\n",
      "Imputed numeric column 'so2_roll_mean_24' with median = 0.37511574074074094\n",
      "Imputed numeric column 'co_lag1' with median = 0.2920555555555555\n",
      "Imputed numeric column 'co_lag3' with median = 0.2920000000000002\n",
      "Imputed numeric column 'co_lag6' with median = 0.2920000000000002\n",
      "Imputed numeric column 'co_lag24' with median = 0.291\n",
      "Imputed numeric column 'co_diff_lag1' with median = 0.0\n",
      "Imputed numeric column 'co_roll_mean_3' with median = 0.2923888888888889\n",
      "Imputed numeric column 'co_roll_mean_6' with median = 0.2927916666666668\n",
      "Imputed numeric column 'co_roll_mean_24' with median = 0.30224652777777783\n",
      "Imputed numeric column 'o3_lag1' with median = 0.034\n",
      "Imputed numeric column 'o3_lag3' with median = 0.034\n",
      "Imputed numeric column 'o3_lag6' with median = 0.034\n",
      "Imputed numeric column 'o3_lag24' with median = 0.034\n",
      "Imputed numeric column 'o3_diff_lag1' with median = 0.0\n",
      "Imputed numeric column 'o3_roll_mean_3' with median = 0.03383333333333333\n",
      "Imputed numeric column 'o3_roll_mean_6' with median = 0.033749999999999995\n",
      "Imputed numeric column 'o3_roll_mean_24' with median = 0.03389583333333332\n",
      "Imputed numeric column 'total_pollutants_lag1' with median = 22.162555555555574\n",
      "Imputed numeric column 'total_pollutants_lag3' with median = 22.162555555555574\n",
      "Imputed numeric column 'total_pollutants_lag6' with median = 22.148888888888905\n",
      "Imputed numeric column 'total_pollutants_lag24' with median = 21.949888888888925\n",
      "Imputed numeric column 'total_pollutants_diff_lag1' with median = 0.04733333333333434\n",
      "Imputed numeric column 'total_pollutants_roll_mean_3' with median = 22.19261111111114\n",
      "Imputed numeric column 'total_pollutants_roll_mean_6' with median = 22.204068672839522\n",
      "Imputed numeric column 'total_pollutants_roll_mean_24' with median = 22.795636882716074\n",
      "Parsed datetime-like column 'timestamp' to datetime dtype (NaT if parse fails)\n",
      "Parsed datetime-like column 'date' to datetime dtype (NaT if parse fails)\n",
      "Creating backup of original table in DB...\n",
      "Backup table created: merged_table_engineered_backup_20251004_011829\n",
      "Writing imputed table back to DB, replacing original table...\n",
      "Replaced table: merged_table_engineered\n",
      "Saved imputed CSV to: merged_engineered_imputed.csv\n",
      "Done. Imputation complete. Original backed up as: merged_table_engineered_backup_20251004_011829\n",
      "Final shape: (2184, 66)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DB_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\"                       # path to your DB\n",
    "TABLE = \"merged_table_engineered\"               # table to modify\n",
    "IMPUTED_TABLE = TABLE                     # we will replace TABLE after backup\n",
    "CSV_OUT = \"merged_engineered_imputed.csv\" # local CSV export for inspection\n",
    "# ----------------\n",
    "\n",
    "def read_table(db_path, table):\n",
    "    if not os.path.isfile(db_path):\n",
    "        raise FileNotFoundError(f\"DB not found: {db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(f'SELECT * FROM \"{table}\";', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def write_table(db_path, table, df, if_exists='replace'):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df.to_sql(table, conn, if_exists=if_exists, index=False)\n",
    "    conn.close()\n",
    "\n",
    "def backup_table(db_path, table):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_name = f\"{table}_backup_{ts}\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    # if backup already exists (very unlikely), drop it\n",
    "    cur.execute(f\"DROP TABLE IF EXISTS \\\"{backup_name}\\\";\")\n",
    "    # create backup as copy\n",
    "    cur.execute(f'CREATE TABLE \"{backup_name}\" AS SELECT * FROM \"{table}\";')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return backup_name\n",
    "\n",
    "def is_datetime_series(s: pd.Series, sample_n=50):\n",
    "    # heuristics: pandas dtype datetime, or many values parseable to datetimes\n",
    "    if np.issubdtype(s.dtype, np.datetime64):\n",
    "        return True\n",
    "    # check sample\n",
    "    non_null = s.dropna().astype(str).head(sample_n)\n",
    "    if len(non_null) == 0:\n",
    "        return False\n",
    "    parseable = 0\n",
    "    for v in non_null:\n",
    "        try:\n",
    "            pd.to_datetime(v)\n",
    "            parseable += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    return (parseable / len(non_null)) > 0.6\n",
    "\n",
    "def main():\n",
    "    print(\"Loading table:\", TABLE, \"from DB:\", DB_PATH)\n",
    "    df = read_table(DB_PATH, TABLE)\n",
    "    if df.empty:\n",
    "        print(\"Table is empty or not found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    # normalize column names (strip)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # 1) Drop no2_wind if exists\n",
    "    if \"no2_wind\" in df.columns:\n",
    "        df = df.drop(columns=[\"no2_wind\"])\n",
    "        print(\"Dropped column: no2_wind\")\n",
    "    else:\n",
    "        print(\"Column 'no2_wind' not present â€” nothing to drop.\")\n",
    "\n",
    "    # 2) Detect column types and plan imputations\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Also detect datetime-like columns among object columns\n",
    "    datetime_cols = [c for c in df.columns if is_datetime_series(df[c])]\n",
    "    # Remove detected datetime columns from object_cols (don't impute them as categoricals)\n",
    "    object_cols = [c for c in object_cols if c not in datetime_cols]\n",
    "\n",
    "    print(\"Numeric columns:\", numeric_cols)\n",
    "    print(\"Datetime-like columns:\", datetime_cols)\n",
    "    print(\"Object/categorical columns to impute with mode:\", object_cols)\n",
    "\n",
    "    # 3) Impute numeric cols with median\n",
    "    for c in numeric_cols:\n",
    "        median_val = df[c].median(skipna=True)\n",
    "        if pd.isna(median_val):\n",
    "            # if median cannot be computed (all NaN), fill with 0\n",
    "            median_val = 0\n",
    "        df[c] = df[c].fillna(median_val)\n",
    "        print(f\"Imputed numeric column '{c}' with median = {median_val}\")\n",
    "\n",
    "    # 4) Impute object/categorical columns with mode (most frequent)\n",
    "    for c in object_cols:\n",
    "        try:\n",
    "            mode_series = df[c].mode(dropna=True)\n",
    "            if not mode_series.empty:\n",
    "                mode_val = mode_series.iloc[0]\n",
    "            else:\n",
    "                mode_val = \"\"\n",
    "        except Exception:\n",
    "            mode_val = \"\"\n",
    "        df[c] = df[c].fillna(mode_val)\n",
    "        # also replace empty strings consisting of only whitespace with mode_val\n",
    "        df[c] = df[c].replace(r'^\\s*$', mode_val, regex=True)\n",
    "        print(f\"Imputed object column '{c}' with mode (or '') = {repr(mode_val)}\")\n",
    "\n",
    "    # 5) For datetime columns: leave as is (but try to parse to datetime dtype)\n",
    "    for c in datetime_cols:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "        print(f\"Parsed datetime-like column '{c}' to datetime dtype (NaT if parse fails)\")\n",
    "\n",
    "    # 6) Backup original table in DB then replace with imputed\n",
    "    print(\"Creating backup of original table in DB...\")\n",
    "    backup_name = backup_table(DB_PATH, TABLE)\n",
    "    print(\"Backup table created:\", backup_name)\n",
    "\n",
    "    print(\"Writing imputed table back to DB, replacing original table...\")\n",
    "    write_table(DB_PATH, TABLE, df, if_exists='replace')\n",
    "    print(\"Replaced table:\", TABLE)\n",
    "\n",
    "    # 7) Save local CSV for quick inspection (optional, user said earlier not necessary; left here for convenience)\n",
    "    try:\n",
    "        df.to_csv(CSV_OUT, index=False)\n",
    "        print(\"Saved imputed CSV to:\", CSV_OUT)\n",
    "    except Exception as e:\n",
    "        print(\"Could not save CSV (continuing):\", e)\n",
    "\n",
    "    print(\"Done. Imputation complete. Original backed up as:\", backup_name)\n",
    "    print(\"Final shape:\", df.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db469303",
   "metadata": {},
   "source": [
    "### Puting final table in database and preparing training csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c48710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded, shape: (2184, 66)\n",
      "Columns: ['timestamp', 'id_ground', 'latitude', 'longitude', 'pm25', 'no2', 'so2', 'o3', 'co', 'date', 'id_weather', 'temperature', 'wind_speed', 'hour', 'dayofweek', 'is_weekend', 'total_pollutants', 'pm25_temp', 'no2_lag1', 'no2_lag3', 'no2_lag6', 'no2_lag24', 'no2_diff_lag1', 'no2_roll_mean_3', 'no2_roll_mean_6', 'no2_roll_mean_24', 'pm25_lag1', 'pm25_lag3', 'pm25_lag6', 'pm25_lag24', 'pm25_diff_lag1', 'pm25_roll_mean_3', 'pm25_roll_mean_6', 'pm25_roll_mean_24', 'so2_lag1', 'so2_lag3', 'so2_lag6', 'so2_lag24', 'so2_diff_lag1', 'so2_roll_mean_3', 'so2_roll_mean_6', 'so2_roll_mean_24', 'co_lag1', 'co_lag3', 'co_lag6', 'co_lag24', 'co_diff_lag1', 'co_roll_mean_3', 'co_roll_mean_6', 'co_roll_mean_24', 'o3_lag1', 'o3_lag3', 'o3_lag6', 'o3_lag24', 'o3_diff_lag1', 'o3_roll_mean_3', 'o3_roll_mean_6', 'o3_roll_mean_24', 'total_pollutants_lag1', 'total_pollutants_lag3', 'total_pollutants_lag6', 'total_pollutants_lag24', 'total_pollutants_diff_lag1', 'total_pollutants_roll_mean_3', 'total_pollutants_roll_mean_6', 'total_pollutants_roll_mean_24']\n",
      "âœ… Table 'final_table' written to C:\\Users\\HP\\Desktop\\suraj\\gdf.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === CONFIG ===\n",
    "CSV_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\merged_engineered_imputed.csv\"    # path to your CSV\n",
    "DB_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\"             # path to your SQLite DB\n",
    "TABLE_NAME = \"final_table\"      # name of table to create/replace\n",
    "# =================\n",
    "\n",
    "def main():\n",
    "    if not os.path.isfile(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "    if not os.path.isfile(DB_PATH):\n",
    "        print(f\"WARNING: DB not found, creating new one at {DB_PATH}\")\n",
    "\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    print(\"CSV loaded, shape:\", df.shape)\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "    # Connect to DB\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "    # Write DataFrame to SQLite (replace if table already exists)\n",
    "    df.to_sql(TABLE_NAME, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"âœ… Table '{TABLE_NAME}' written to {DB_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f805481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2184 rows to C:\\Users\\HP\\Desktop\\suraj\\gdf.db -> table 'final_file'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "CSV_PATH = r\"C:\\Users\\HP\\Desktop\\suraj\\merged_engineered_imputed.csv\"   # update if file is elsewhere\n",
    "DB_PATH  = r\"C:\\Users\\HP\\Desktop\\suraj\\gdf.db\"\n",
    "TABLE    = \"final_file\"                  # table name to create/replace\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    df.to_sql(TABLE, conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(f\"Wrote {len(df)} rows to {DB_PATH} -> table '{TABLE}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
