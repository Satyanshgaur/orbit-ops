{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49338168",
   "metadata": {},
   "source": [
    "## TEMPO NO2 Data Download and Processing for NYC\n",
    "Downloads TEMPO satellite NO2 data and converts to CSV for ML training\n",
    "\n",
    "Data Specifications\n",
    "TEMPO NO2 L3 Product Details\n",
    "\n",
    "Spatial Resolution: ~2 km at nadir\n",
    "Temporal Resolution: Hourly during daylight (typically 12-14 scans/day)\n",
    "Coverage: Continental US, Mexico, and most of Canada\n",
    "Key Variables:\n",
    "\n",
    "Tropospheric NO2 column (primary for air quality)\n",
    "Stratospheric NO2 column\n",
    "Total column NO2\n",
    "Quality flags and uncertainties\n",
    "\n",
    "\n",
    "\n",
    "Expected Data Volume\n",
    "\n",
    "NYC area: ~200-300 grid cells\n",
    "2 weeks of data: ~200-300 files\n",
    "Each file: ~5-20 MB\n",
    "Total download: ~1-5 GB\n",
    "Processed CSV: ~10-50 MB\n",
    "\n",
    "Next Steps for ML Training\n",
    "After downloading data, you'll have:\n",
    "\n",
    "Spatial CSV: Use for spatial modeling or as features\n",
    "Time Series CSV: Use for temporal forecasting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3aa05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Satyansh Gaur\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import earthaccess\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5a5d7",
   "metadata": {},
   "source": [
    "### Configuration \n",
    "configuring the boundary boxes for, latitude and longitude range for New York city. Setting date and time for downloading the data. Creating directories to save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bca598",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2023-04-01\"\n",
    "END_DATE = \"2023-07-01\"  \n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"tempo_data\")\n",
    "NETCDF_DIR = OUTPUT_DIR / \"netcdf\"\n",
    "CSV_DIR = OUTPUT_DIR / \"csv\"\n",
    "\n",
    "# Create directories\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "NETCDF_DIR.mkdir(exist_ok=True)\n",
    "CSV_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f4cee",
   "metadata": {},
   "source": [
    "### STEP 1: EARTHDATA AUTHENTICATION SETUP\n",
    "\n",
    "Setup authentication for NASA Earthdata.\n",
    "    \n",
    "First time users: Run this to create credentials\n",
    "You'll need to:\n",
    "1. Create account at: https://urs.earthdata.nasa.gov/\n",
    "2. Run earthaccess.login() which will prompt for username/password\n",
    "3. Credentials will be saved for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540b8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_earthdata_auth():\n",
    "  \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EARTHDATA AUTHENTICATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Try to login (will use saved credentials if available)\n",
    "        auth = earthaccess.login(strategy=\"interactive\")\n",
    "        \n",
    "        if auth.authenticated:\n",
    "            print(\"✓ Successfully authenticated with NASA Earthdata!\")\n",
    "            return auth\n",
    "        else:\n",
    "            print(\"✗ Authentication failed. Please check your credentials.\")\n",
    "            print(\"\\nTo create an account:\")\n",
    "            print(\"1. Visit: https://urs.earthdata.nasa.gov/\")\n",
    "            print(\"2. Click 'Register' and follow the steps\")\n",
    "            print(\"3. Run this script again\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during authentication: {e}\")\n",
    "        print(\"\\nFirst time setup:\")\n",
    "        print(\"1. Create account at: https://urs.earthdata.nasa.gov/\")\n",
    "        print(\"2. Run: earthaccess.login() in Python\")\n",
    "        print(\"3. Enter your username and password when prompted\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfa3c9",
   "metadata": {},
   "source": [
    "### STEP 2: SEARCH FOR TEMPO DATA\n",
    " Search for TEMPO NO2 Level 3 data.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date string (YYYY-MM-DD)\n",
    "        end_date: End date string (YYYY-MM-DD)\n",
    "        bounds: Dictionary with min_lon, max_lon, min_lat, max_lat\n",
    "    \n",
    "    Returns:\n",
    "        List of granules found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b0c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tempo_data(start_date, end_date, bounds=None):\n",
    " \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SEARCHING FOR TEMPO DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Date Range: {start_date} to {end_date}\")\n",
    "    if bounds:\n",
    "        print(f\"Spatial Bounds: {bounds}\")\n",
    "    \n",
    "    try:\n",
    "        # Search for TEMPO NO2 L3 data\n",
    "        search_params = {\n",
    "            'short_name': 'TEMPO_NO2_L3',\n",
    "            'temporal': (start_date, end_date)\n",
    "        }\n",
    "        \n",
    "        # Add spatial bounds if provided\n",
    "        if bounds:\n",
    "            search_params['bounding_box'] = (\n",
    "                bounds['min_lon'],\n",
    "                bounds['min_lat'],\n",
    "                bounds['max_lon'],\n",
    "                bounds['max_lat']\n",
    "            )\n",
    "        \n",
    "        results = earthaccess.search_data(**search_params)\n",
    "        \n",
    "        print(f\"\\n✓ Found {len(results)} granules\")\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            print(f\"\\nSample granule info:\")\n",
    "            print(f\"  - Size: {results[0].size()} MB\")\n",
    "            print(f\"  - Format: {results[0].data_links()[0].split('.')[-1]}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error searching for data: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141db55e",
   "metadata": {},
   "source": [
    "### STEP 3: DOWNLOAD DATA\n",
    "    Download TEMPO granules to local directory.\n",
    "    \n",
    "    Args:\n",
    "        granules: List of granules from search\n",
    "        output_dir: Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa25581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_tempo_data(granules, output_dir):\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DOWNLOADING DATA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not granules:\n",
    "        print(\"✗ No granules to download\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Download files\n",
    "        print(f\"Downloading {len(granules)} files to {output_dir}...\")\n",
    "        files = earthaccess.download(\n",
    "            granules,\n",
    "            local_path=str(output_dir)\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Successfully downloaded {len(files)} files\")\n",
    "        return files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a661e",
   "metadata": {},
   "source": [
    "### STEP 4: PROCESS NETCDF TO CSV\n",
    "    Process TEMPO NetCDF files and create CSV for ML training.\n",
    "    \n",
    "    Args:\n",
    "        netcdf_files: List of NetCDF file paths\n",
    "        bounds: Dictionary with spatial bounds for NYC\n",
    "        output_csv: Output CSV file path\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4640aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_netcdf_to_csv(netcdf_files, bounds, output_csv):\n",
    " \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESSING NETCDF TO CSV\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for i, file in enumerate(netcdf_files, 1):\n",
    "        print(f\"\\nProcessing file {i}/{len(netcdf_files)}: {Path(file).name}\")\n",
    "        \n",
    "        try:\n",
    "            # Open NetCDF file\n",
    "            ds = xr.open_dataset(file)\n",
    "            \n",
    "            # Extract key variables\n",
    "            # Check what variables are available\n",
    "            print(f\"  Available variables: {list(ds.data_vars)}\")\n",
    "            \n",
    "            # Subset to NYC area\n",
    "            ds_nyc = ds.sel(\n",
    "                longitude=slice(bounds['min_lon'], bounds['max_lon']),\n",
    "                latitude=slice(bounds['min_lat'], bounds['max_lat'])\n",
    "            )\n",
    "            \n",
    "            # Extract time\n",
    "            if 'time' in ds_nyc.coords:\n",
    "                time_val = pd.to_datetime(ds_nyc.time.values[0])\n",
    "            else:\n",
    "                # Parse from filename or use file attributes\n",
    "                time_val = pd.to_datetime(ds.attrs.get('time_coverage_start', 'NaT'))\n",
    "            \n",
    "            # Extract NO2 data (adjust variable names based on what's available)\n",
    "            variables_to_extract = [\n",
    "                'vertical_column_troposphere',  # Tropospheric NO2\n",
    "                'vertical_column_stratosphere',  # Stratospheric NO2\n",
    "                'vertical_column_total',         # Total column NO2\n",
    "                'column_amount',                 # Alternative name\n",
    "                'tropopause_pressure',          # Useful ancillary data\n",
    "                'surface_pressure',\n",
    "                'cloud_fraction',\n",
    "                'solar_zenith_angle',\n",
    "                'weight'  # Area weight\n",
    "            ]\n",
    "            \n",
    "            # Create dataframe for this file\n",
    "            lons, lats = np.meshgrid(ds_nyc.longitude.values, ds_nyc.latitude.values)\n",
    "            \n",
    "            file_data = {\n",
    "                'timestamp': time_val,\n",
    "                'date': time_val.date() if not pd.isna(time_val) else None,\n",
    "                'hour': time_val.hour if not pd.isna(time_val) else None,\n",
    "                'latitude': lats.flatten(),\n",
    "                'longitude': lons.flatten()\n",
    "            }\n",
    "            \n",
    "            # Extract available variables\n",
    "            for var in variables_to_extract:\n",
    "                if var in ds_nyc.data_vars:\n",
    "                    data = ds_nyc[var].values\n",
    "                    if data.ndim == 3:  # (time, lat, lon)\n",
    "                        data = data[0]  # Take first time step\n",
    "                    file_data[var] = data.flatten()\n",
    "                    print(f\"  ✓ Extracted: {var}\")\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(file_data)\n",
    "            \n",
    "            # Remove rows with all NaN values in data columns\n",
    "            data_cols = [col for col in df.columns if col not in ['timestamp', 'date', 'hour', 'latitude', 'longitude']]\n",
    "            df = df.dropna(subset=data_cols, how='all')\n",
    "            \n",
    "            if len(df) > 0:\n",
    "                all_data.append(df)\n",
    "                print(f\"  ✓ Extracted {len(df)} valid data points\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No valid data points found\")\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing file: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"Combining all data...\")\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        final_df = final_df.sort_values('timestamp')\n",
    "        \n",
    "        # Save to CSV\n",
    "        final_df.to_csv(output_csv, index=False)\n",
    "        print(f\"✓ Saved combined data to: {output_csv}\")\n",
    "        print(f\"  - Total rows: {len(final_df):,}\")\n",
    "        print(f\"  - Columns: {list(final_df.columns)}\")\n",
    "        print(f\"  - Date range: {final_df['timestamp'].min()} to {final_df['timestamp'].max()}\")\n",
    "        \n",
    "        # Display summary statistics\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"DATA SUMMARY\")\n",
    "        print(\"-\" * 70)\n",
    "        print(final_df.describe())\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"✗ No data to combine\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6991677",
   "metadata": {},
   "source": [
    "### STEP 5: CREATE AGGREGATED TIME SERIES\n",
    "   Create aggregated time series (hourly/daily averages) for ML training.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with spatial data\n",
    "        output_csv: Output CSV file path\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0c6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_timeseries(df, output_csv):\n",
    " \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CREATING AGGREGATED TIME SERIES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"✗ No data to aggregate\")\n",
    "        return None\n",
    "    \n",
    "    # Group by timestamp and calculate spatial averages over NYC\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['latitude', 'longitude']]\n",
    "    \n",
    "    agg_dict = {col: ['mean', 'std', 'min', 'max', 'count'] for col in numeric_cols}\n",
    "    \n",
    "    agg_df = df.groupby('timestamp').agg(agg_dict)\n",
    "    \n",
    "    # Flatten column names\n",
    "    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df = agg_df.reset_index()\n",
    "    \n",
    "    # Add time features\n",
    "    agg_df['date'] = agg_df['timestamp'].dt.date\n",
    "    agg_df['hour'] = agg_df['timestamp'].dt.hour\n",
    "    agg_df['day_of_week'] = agg_df['timestamp'].dt.dayofweek\n",
    "    agg_df['is_weekend'] = agg_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Save aggregated data\n",
    "    agg_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✓ Saved aggregated time series to: {output_csv}\")\n",
    "    print(f\"  - Total time points: {len(agg_df)}\")\n",
    "    print(f\"  - Columns: {len(agg_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"AGGREGATED DATA PREVIEW\")\n",
    "    print(\"-\" * 70)\n",
    "    print(agg_df.head(10))\n",
    "    \n",
    "    return agg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b769b",
   "metadata": {},
   "source": [
    "### Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_aggregated_timeseries(df, output_csv):\n",
    "    \"\"\"\n",
    "    Create aggregated time series (hourly/daily averages) for ML training.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with spatial data\n",
    "        output_csv: Output CSV file path\n",
    "    \n",
    "    Returns:\n",
    "        Aggregated DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CREATING AGGREGATED TIME SERIES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"✗ No data to aggregate\")\n",
    "        return None\n",
    "    \n",
    "    # Group by timestamp and calculate spatial averages over NYC\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['latitude', 'longitude']]\n",
    "    \n",
    "    agg_dict = {col: ['mean', 'std', 'min', 'max', 'count'] for col in numeric_cols}\n",
    "    \n",
    "    agg_df = df.groupby('timestamp').agg(agg_dict)\n",
    "    \n",
    "    # Flatten column names\n",
    "    agg_df.columns = ['_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df = agg_df.reset_index()\n",
    "    \n",
    "    # Add time features\n",
    "    agg_df['date'] = agg_df['timestamp'].dt.date\n",
    "    agg_df['hour'] = agg_df['timestamp'].dt.hour\n",
    "    agg_df['day_of_week'] = agg_df['timestamp'].dt.dayofweek\n",
    "    agg_df['is_weekend'] = agg_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Save aggregated data\n",
    "    agg_df.to_csv(output_csv, index=False)\n",
    "    print(f\"✓ Saved aggregated time series to: {output_csv}\")\n",
    "    print(f\"  - Total time points: {len(agg_df)}\")\n",
    "    print(f\"  - Columns: {len(agg_df.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"AGGREGATED DATA PREVIEW\")\n",
    "    print(\"-\" * 70)\n",
    "    print(agg_df.head(10))\n",
    "    \n",
    "    return agg_df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TEMPO NO2 DATA DOWNLOAD AND PROCESSING FOR NYC\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Location: NYC ({NYC_BOUNDS})\")\n",
    "    print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Step 1: Authenticate\n",
    "    auth = setup_earthdata_auth()\n",
    "    if not auth:\n",
    "        print(\"\\n✗ Cannot proceed without authentication\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Search for data\n",
    "    granules = search_tempo_data(START_DATE, END_DATE, NYC_BOUNDS)\n",
    "    if not granules:\n",
    "        print(\"\\n✗ No data found. Check date range and try again.\")\n",
    "        print(\"Note: TEMPO data is only available from 2023 onwards\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Download data\n",
    "    downloaded_files = download_tempo_data(granules, NETCDF_DIR)\n",
    "    if not downloaded_files:\n",
    "        print(\"\\n✗ No files downloaded\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Process to CSV\n",
    "    spatial_csv = CSV_DIR / \"tempo_no2_nyc_spatial.csv\"\n",
    "    df = process_netcdf_to_csv(downloaded_files, NYC_BOUNDS, spatial_csv)\n",
    "    \n",
    "    # Step 5: Create aggregated time series\n",
    "    if df is not None:\n",
    "        aggregated_csv = CSV_DIR / \"tempo_no2_nyc_timeseries.csv\"\n",
    "        agg_df = create_aggregated_timeseries(df, aggregated_csv)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ PROCESSING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  1. NetCDF files: {NETCDF_DIR}\")\n",
    "    print(f\"  2. Spatial CSV: {spatial_csv}\")\n",
    "    print(f\"  3. Time series CSV: {CSV_DIR / 'tempo_no2_nyc_timeseries.csv'}\")\n",
    "    print(\"\\nYou can now use the CSV files for ML training!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf06fb9",
   "metadata": {},
   "source": [
    "### Download Multiple Regions\n",
    "Run the script multiple times with different bounds and output directories.\n",
    "Support and Resources\n",
    "\n",
    "TEMPO Documentation: https://tempo.si.edu/\n",
    "Earthdata Search: https://search.earthdata.nasa.gov/\n",
    "earthaccess Docs: https://earthaccess.readthedocs.io/\n",
    "TEMPO Data Guide: https://asdc.larc.nasa.gov/project/TEMPO\n",
    "\n",
    "### Trobleshooting checklist\n",
    "\n",
    " NASA Earthdata account created\n",
    " All packages installed (pip list | grep earthaccess)\n",
    " Authenticated successfully (earthaccess.login())\n",
    " Date range is valid (2023 onwards)\n",
    " Sufficient disk space (~5-10 GB)\n",
    " Internet connection stable\n",
    " NYC bounds configured correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
